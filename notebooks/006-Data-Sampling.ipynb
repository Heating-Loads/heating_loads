{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940ac45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f869fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0792a659",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9ea538",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data = pd.read_csv(path + 'data_btu_railbelt.csv')\n",
    "joined_data = joined_data.drop(columns =['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab6295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4795f8d1",
   "metadata": {},
   "source": [
    "# even split between fairbanks and anchorage--downsample anchorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413e57e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fairbanks\n",
    "zip_fb = [99701, 99702, 99703, 99705, 99706, 99707, 99708, 99709, 99710, \n",
    "          99711, 99712, 99714, 99716, 99725, 99775, 99790]\n",
    "\n",
    "# Anchorage\n",
    "zip_an = [99501, 99502, 99503, 99504, 99505, 99506, 99507, 99508, 99509, 99510,\n",
    "           99511, 99513, 99514, 99515, 99516, 99517, 99518, 99519, 99520, 99521,\n",
    "           99522, 99523, 99524, 99529, 99530, 99540, 99599, 99695]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8151f6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairbanks_only = joined_data[joined_data['zip_code'].isin(zip_fb)]\n",
    "anchorage_only = joined_data[joined_data['zip_code'].isin(zip_an)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0824332d",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchorage_ind_keep = np.random.choice(len(anchorage_only), len(fairbanks_only)).tolist()\n",
    "new_anchorage = anchorage_only.iloc[anchorage_ind_keep, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf31311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined data with augemented fairbanks data\n",
    "combined_fair_anch = new_anchorage.copy(deep=True)\n",
    "combined_fair_anch = combined_fair_anch.append(fairbanks_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1517ee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change FD cols to be positive\n",
    "combined_fair_anch['FD_1981_2010'] = abs(combined_fair_anch['FD_1981_2010'])\n",
    "combined_fair_anch['FD_2000'] = abs(combined_fair_anch['FD_2000'])\n",
    "combined_fair_anch['FD_2010'] = abs(combined_fair_anch['FD_2010'])\n",
    "combined_fair_anch['FD_90'] = abs(combined_fair_anch['FD_90'])\n",
    "combined_fair_anch['FD_80'] = abs(combined_fair_anch['FD_80'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba07be56",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_fairbanks = combined_fair_anch[combined_fair_anch['zip_code'].isin(zip_fb)]\n",
    "Xy_anchorage = combined_fair_anch[combined_fair_anch['zip_code'].isin(zip_an)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3736bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create numpy arrays for scikit learn \n",
    "features_dropped = ['annual_btu_persqft', 'annual_btu', 'age', 'osm_id', 'zip_code', 'zip_group']\n",
    "# features_kept = combined_fair_anch.drop(features_dropped, axis=1).columns\n",
    "\n",
    "X_fair = np.array(Xy_fairbanks.drop(features_dropped, axis=1))\n",
    "y_fair = np.array(Xy_fairbanks['annual_btu_persqft'])\n",
    "y_fair = y_fair.reshape([y_fair.shape[0],1])\n",
    "\n",
    "X_anch = np.array(Xy_anchorage.drop(features_dropped, axis=1))\n",
    "y_anch = np.array(Xy_anchorage['annual_btu_persqft'])\n",
    "y_anch = y_anch.reshape([y_anch.shape[0],1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fea9f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(np.vstack((X_fair, X_anch)))\n",
    "X_fair = scaler.transform(X_fair)\n",
    "X_anch = scaler.transform(X_anch)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(np.vstack((y_fair, y_anch)))\n",
    "y_fair = scaler.transform(y_fair)\n",
    "y_anch = scaler.transform(y_anch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd01bc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide intro training and testing datasets, with a 30-70 ratio\n",
    "X_train_anch, X_test_anch, y_train_anch, y_test_anch = train_test_split(X_anch, y_anch, test_size = 0.3, random_state = 1)\n",
    "\n",
    "X_train_fair, X_test_fair, y_train_fair, y_test_fair = train_test_split(X_fair, y_fair, test_size = 0.3, random_state = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2361fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine fairbanks and anchorage\n",
    "X_train = np.vstack((X_train_anch, X_train_fair))\n",
    "X_test = np.vstack((X_test_anch, X_test_fair))\n",
    "y_train = np.vstack((y_train_anch, y_train_fair))\n",
    "y_test = np.vstack((y_test_anch, y_test_fair))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0ef41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'data_sampling/equal_anch_fair/x_train_downsample_anchorage.npy', 'wb') as f:\n",
    "    np.save(f, X_train)\n",
    "\n",
    "    \n",
    "with open(path + 'data_sampling/equal_anch_fair/x_test_downsample_anchorage.npy', 'wb') as f:\n",
    "    np.save(f, X_test)\n",
    "    \n",
    "    \n",
    "with open(path + 'data_sampling/equal_anch_fair/y_train_downsample_anchorage.npy', 'wb') as f:\n",
    "    np.save(f, y_train)\n",
    "    \n",
    "with open(path + 'data_sampling/equal_anch_fair/y_test_downsample_anchorage.npy', 'wb') as f:\n",
    "    np.save(f, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f92a42f",
   "metadata": {},
   "source": [
    "# even split between fairbanks and anchorage--upsample fairbanks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79a6777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fairbanks\n",
    "zip_fb = [99701, 99702, 99703, 99705, 99706, 99707, 99708, 99709, 99710, \n",
    "          99711, 99712, 99714, 99716, 99725, 99775, 99790]\n",
    "\n",
    "# Anchorage\n",
    "zip_an = [99501, 99502, 99503, 99504, 99505, 99506, 99507, 99508, 99509, 99510,\n",
    "           99511, 99513, 99514, 99515, 99516, 99517, 99518, 99519, 99520, 99521,\n",
    "           99522, 99523, 99524, 99529, 99530, 99540, 99599, 99695]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6584d08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairbanks_only = joined_data[joined_data['zip_code'].isin(zip_fb)]\n",
    "anchorage_only = joined_data[joined_data['zip_code'].isin(zip_an)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecade0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fairbanks = fairbanks_only.copy(deep=True)\n",
    "num_samples = len(anchorage_only) - len(fairbanks_only)\n",
    "new_fairbanks = new_fairbanks.append(fairbanks_only.sample(n = num_samples, replace = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ab0cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fairbanks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba76e237",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# combined data with augemented fairbanks data\n",
    "combined_fair_anch = new_fairbanks.copy(deep=True)\n",
    "combined_fair_anch = combined_fair_anch.append(anchorage_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be269b98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_fair_anch.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f471aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_fair_anch.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae959381",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_fair_anch.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd403ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change FD cols to be positive\n",
    "combined_fair_anch['FD_1981_2010'] = abs(combined_fair_anch['FD_1981_2010'])\n",
    "combined_fair_anch['FD_2000'] = abs(combined_fair_anch['FD_2000'])\n",
    "combined_fair_anch['FD_2010'] = abs(combined_fair_anch['FD_2010'])\n",
    "combined_fair_anch['FD_90'] = abs(combined_fair_anch['FD_90'])\n",
    "combined_fair_anch['FD_80'] = abs(combined_fair_anch['FD_80'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bd1b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_fairbanks = combined_fair_anch[combined_fair_anch['zip_code'].isin(zip_fb)]\n",
    "Xy_anchorage = combined_fair_anch[combined_fair_anch['zip_code'].isin(zip_an)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb452bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create numpy arrays for scikit learn \n",
    "features_dropped = ['annual_btu_persqft', 'annual_btu', 'age', 'osm_id', 'zip_code', 'zip_group']\n",
    "# features_kept = combined_fair_anch.drop(features_dropped, axis=1).columns\n",
    "\n",
    "X_fair = np.array(Xy_fairbanks.drop(features_dropped, axis=1))\n",
    "y_fair = np.array(Xy_fairbanks['annual_btu_persqft'])\n",
    "y_fair = y_fair.reshape([y_fair.shape[0],1])\n",
    "\n",
    "X_anch = np.array(Xy_anchorage.drop(features_dropped, axis=1))\n",
    "y_anch = np.array(Xy_anchorage['annual_btu_persqft'])\n",
    "y_anch = y_anch.reshape([y_anch.shape[0],1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dc90b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(np.vstack((X_fair, X_anch)))\n",
    "X_fair = scaler.transform(X_fair)\n",
    "X_anch = scaler.transform(X_anch)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(np.vstack((y_fair, y_anch)))\n",
    "y_fair = scaler.transform(y_fair)\n",
    "y_anch = scaler.transform(y_anch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb66ab57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide intro training and testing datasets, with a 30-70 ratio\n",
    "X_train_anch, X_test_anch, y_train_anch, y_test_anch = train_test_split(X_anch, y_anch, test_size = 0.3, random_state = 1)\n",
    "\n",
    "X_train_fair, X_test_fair, y_train_fair, y_test_fair = train_test_split(X_fair, y_fair, test_size = 0.3, random_state = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d256046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine fairbanks and anchorage\n",
    "X_train = np.vstack((X_train_anch, X_train_fair))\n",
    "X_test = np.vstack((X_test_anch, X_test_fair))\n",
    "y_train = np.vstack((y_train_anch, y_train_fair))\n",
    "y_test = np.vstack((y_test_anch, y_test_fair))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aaa3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'data_sampling/equal_anch_fair/x_train_upsample_fairbanks.npy', 'wb') as f:\n",
    "    np.save(f, X_train)\n",
    "\n",
    "    \n",
    "with open(path + 'data_sampling/equal_anch_fair/x_test_upsample_fairbanks.npy', 'wb') as f:\n",
    "    np.save(f, X_test)\n",
    "    \n",
    "    \n",
    "with open(path + 'data_sampling/equal_anch_fair/y_train_upsample_fairbanks.npy', 'wb') as f:\n",
    "    np.save(f, y_train)\n",
    "    \n",
    "with open(path + 'data_sampling/equal_anch_fair/y_test_upsample_fairbanks.npy', 'wb') as f:\n",
    "    np.save(f, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cbd6d8",
   "metadata": {},
   "source": [
    "# Building age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a53ec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data = pd.read_csv(path + 'data_btu_railbelt.csv')\n",
    "joined_data = joined_data.drop(columns =['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8adaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin age\n",
    "joined_data['year_rounded'] = abs(joined_data['age'].round() - 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdb060f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change FD cols to be positive\n",
    "joined_data['FD_1981_2010'] = abs(joined_data['FD_1981_2010'])\n",
    "joined_data['FD_2000'] = abs(joined_data['FD_2000'])\n",
    "joined_data['FD_2010'] = abs(joined_data['FD_2010'])\n",
    "joined_data['FD_90'] = abs(joined_data['FD_90'])\n",
    "joined_data['FD_80'] = abs(joined_data['FD_80'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b81005",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bf3196",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data_nonmissing = joined_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd51077",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data_nonmissing['year_rounded'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c4dcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "years_df_dict = {}\n",
    "for year in joined_data_nonmissing['year_rounded'].unique():\n",
    "    years_df_dict[year] = joined_data_nonmissing[joined_data_nonmissing['year_rounded'] == year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130027ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_of_bins = []\n",
    "for key in years_df_dict.keys():\n",
    "    size_of_bins.append(len(years_df_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121f0c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(size_of_bins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6520aaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_count = 0\n",
    "for year in years_df_dict.keys():\n",
    "    #print('year: ', year)\n",
    "    #print(len(years_df_dict[year]))\n",
    "    \n",
    "    if len(years_df_dict[year])> max_count:\n",
    "        max_count = len(years_df_dict[year])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8758fece",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354cc59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# up and down sample to 5000\n",
    "num_samples = 2200 #5000\n",
    "sampled_dfs = {}\n",
    "for year in years_df_dict.keys():\n",
    "    current_df = years_df_dict[year]\n",
    "    num_buildings = len(current_df)\n",
    "    new_df = pd.DataFrame()\n",
    "    \n",
    "    if num_buildings > num_samples:\n",
    "        # downsample\n",
    "        new_df = new_df.append(current_df.sample(n=num_samples)) \n",
    "    elif num_buildings < num_samples:\n",
    "        # upsample\n",
    "        new_df = new_df.append(current_df)\n",
    "        new_df = new_df.append(current_df.sample(n=num_samples - len(current_df), replace=True))\n",
    "    else:\n",
    "        new_df = current_df\n",
    "    \n",
    "    sampled_dfs[year] = new_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f340d3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create numpy arrays for scikit learn \n",
    "features_dropped = ['annual_btu_persqft', 'annual_btu', 'age', 'osm_id', 'zip_code', 'zip_group', 'year_rounded']\n",
    "# features_kept = combined_fair_anch.drop(features_dropped, axis=1).columns\n",
    "\n",
    "X_list = []\n",
    "y_list = []\n",
    "for year in sampled_dfs.keys():\n",
    "    df = sampled_dfs[year]\n",
    "    X = np.array(df.drop(features_dropped, axis=1))\n",
    "    y = np.array(df['annual_btu_persqft'])\n",
    "    y = y.reshape([y.shape[0],1])\n",
    "    \n",
    "    X_list.append(X)\n",
    "    y_list.append(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b75288",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_dfs[0].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8f0cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(np.vstack((X_list)))\n",
    "X_list = [scaler.transform(x_elem) for x_elem in X_list]\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(np.vstack((y_list)))\n",
    "y_list = [scaler.transform(y_elem) for y_elem in y_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9f77f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list = []\n",
    "X_test_list = []\n",
    "y_train_list = []\n",
    "y_test_list = []\n",
    "for i in range(len(X_list)):\n",
    "    X = X_list[i]\n",
    "    y = y_list[i]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1)\n",
    "    \n",
    "    X_train_list.append(X_train)\n",
    "    X_test_list.append(X_test)\n",
    "    y_train_list.append(y_train)\n",
    "    y_test_list.append(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abc37f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine ages\n",
    "X_train = np.vstack((X_train_list))\n",
    "X_test = np.vstack((X_test_list))\n",
    "y_train = np.vstack((y_train_list))\n",
    "y_test = np.vstack((y_test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9c95f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'data_sampling/equal_age/x_train_2200.npy', 'wb') as f:\n",
    "    np.save(f, X_train)\n",
    "\n",
    "    \n",
    "with open(path + 'data_sampling/equal_age/x_test_2200.npy', 'wb') as f:\n",
    "    np.save(f, X_test)\n",
    "    \n",
    "    \n",
    "with open(path + 'data_sampling/equal_age/y_train_2200.npy', 'wb') as f:\n",
    "    np.save(f, y_train)\n",
    "    \n",
    "with open(path + 'data_sampling/equal_age/y_test.npy_2200', 'wb') as f:\n",
    "    np.save(f, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc250b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c07b3bce",
   "metadata": {},
   "source": [
    "# equal sq ft-- unsure about how to choose the bins here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e631b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data = pd.read_csv(path + 'data_btu_railbelt.csv')\n",
    "joined_data = joined_data.drop(columns =['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936b1355",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data_nonmissing = joined_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571366a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.cut(np.array([1, 7, 5, 4, 6, 3]), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaaae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data_nonmissing['sq_ft_bin'] = pd.qcut(joined_data_nonmissing['areasq_ft'], 50, labels = np.arange(50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b91659",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data_nonmissing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3ecf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    print(len(joined_data_nonmissing[joined_data_nonmissing['sq_ft_bin'] == i]))\n",
    "    print(joined_data_nonmissing[joined_data_nonmissing['sq_ft_bin'] ==i]['sq_ft_bin'].max())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6d2284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # bin sq ft\n",
    "# joined_data['year_rounded'] = abs(joined_data['age'].round() - 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf92bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # change FD cols to be positive\n",
    "# joined_data['FD_1981_2010'] = abs(joined_data['FD_1981_2010'])\n",
    "# joined_data['FD_2000'] = abs(joined_data['FD_2000'])\n",
    "# joined_data['FD_2010'] = abs(joined_data['FD_2010'])\n",
    "# joined_data['FD_90'] = abs(joined_data['FD_90'])\n",
    "# joined_data['FD_80'] = abs(joined_data['FD_80'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8854d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joined_data_nonmissing['year_rounded'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab0661d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# years_df_dict = {}\n",
    "# for year in joined_data_nonmissing['year_rounded'].unique():\n",
    "#     years_df_dict[year] = joined_data_nonmissing[joined_data_nonmissing['year_rounded'] == year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c5087c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_count = 0\n",
    "# for year in years_df_dict.keys():\n",
    "#     #print('year: ', year)\n",
    "#     #print(len(years_df_dict[year]))\n",
    "    \n",
    "#     if len(years_df_dict[year])> max_count:\n",
    "#         max_count = len(years_df_dict[year])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5cda25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # up and down sample to 5000\n",
    "# num_samples = 5000\n",
    "# sampled_dfs = {}\n",
    "# for year in years_df_dict.keys():\n",
    "#     current_df = years_df_dict[year]\n",
    "#     num_buildings = len(current_df)\n",
    "#     new_df = pd.DataFrame()\n",
    "    \n",
    "#     if num_buildings > num_samples:\n",
    "#         # downsample\n",
    "#         new_df = new_df.append(current_df.sample(n=num_samples)) \n",
    "#     elif num_buildings < num_samples:\n",
    "#         # upsample\n",
    "#         new_df = new_df.append(current_df)\n",
    "#         new_df = new_df.append(current_df.sample(n=num_samples - len(current_df), replace=True))\n",
    "#     else:\n",
    "#         new_df = current_df\n",
    "    \n",
    "#     sampled_dfs[year] = new_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a10663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create numpy arrays for scikit learn \n",
    "# features_dropped = ['annual_btu_persqft', 'annual_btu', 'age', 'osm_id', 'zip_code', 'zip_group', 'sq_ft_bin']\n",
    "# # features_kept = combined_fair_anch.drop(features_dropped, axis=1).columns\n",
    "\n",
    "# X_list = []\n",
    "# y_list = []\n",
    "# for year in sampled_dfs.keys():\n",
    "#     df = sampled_dfs[year]\n",
    "#     X = np.array(df.drop(features_dropped, axis=1))\n",
    "#     y = np.array(df['annual_btu_persqft'])\n",
    "#     y = y.reshape([y.shape[0],1])\n",
    "    \n",
    "#     X_list.append(X)\n",
    "#     y_list.append(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ba2828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # standardize data\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(np.vstack((X_list)))\n",
    "# X_list = [scaler.transform(x_elem) for x_elem in X_list]\n",
    "\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(np.vstack((y_list)))\n",
    "# y_list = [scaler.transform(y_elem) for y_elem in y_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c06e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_list = []\n",
    "# X_test_list = []\n",
    "# y_train_list = []\n",
    "# y_test_list = []\n",
    "# for i in range(len(X_list)):\n",
    "#     X = X_list[i]\n",
    "#     y = y_list[i]\n",
    "    \n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1)\n",
    "    \n",
    "#     X_train_list.append(X_train)\n",
    "#     X_test_list.append(X_test)\n",
    "#     y_train_list.append(y_train)\n",
    "#     y_test_list.append(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332b086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine ages\n",
    "# X_train = np.vstack((X_train_list))\n",
    "# X_test = np.vstack((X_test_list))\n",
    "# y_train = np.vstack((y_train_list))\n",
    "# y_test = np.vstack((y_test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884a59be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(path + 'data_sampling/equal_age/x_train.npy', 'wb') as f:\n",
    "#     np.save(f, X_train)\n",
    "\n",
    "    \n",
    "# with open(path + 'data_sampling/equal_age/x_test.npy', 'wb') as f:\n",
    "#     np.save(f, X_test)\n",
    "    \n",
    "    \n",
    "# with open(path + 'data_sampling/equal_age/y_train.npy', 'wb') as f:\n",
    "#     np.save(f, y_train)\n",
    "    \n",
    "# with open(path + 'data_sampling/equal_age/y_test.npy', 'wb') as f:\n",
    "#     np.save(f, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8834d087",
   "metadata": {},
   "source": [
    "# all regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c946685d",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1993b306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train,y_train)\n",
    "\n",
    "y_pred_lm = lm.predict(X_test) # predicting using train dataset\n",
    "\n",
    "coeffs = lm.coef_\n",
    "print(coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da61989",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_lm = mean_squared_error(y_test, y_pred_lm)\n",
    "print(\"MSE model_linear:\", mse_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85292eb",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731f66a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression\n",
    "ridge = Ridge(alpha= 0.5)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "coeffs = ridge.coef_\n",
    "print(coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4979b84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "print(\"MSE model_ridge:\", mse_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfc0b84",
   "metadata": {},
   "source": [
    "## Ridge Regression with Cross Validation--?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659b14a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression with CV and with polynomial degree 2 features\n",
    "\n",
    "# Polynomial Fit\n",
    "\n",
    "poly_train = PolynomialFeatures(2)\n",
    "X_train_poly = poly_train.fit_transform(X_train)\n",
    "\n",
    "poly_test = PolynomialFeatures(2)\n",
    "X_test_poly = poly_test.fit_transform(X_test)\n",
    "\n",
    "ridgeCV_poly = RidgeCV(cv=10).fit(X_train_poly,y_train)\n",
    "y_pred_ridgeCV_poly = ridgeCV_poly.predict(X_test_poly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3f8114",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_ridgeCV_poly = mean_squared_error(y_test, y_pred_ridgeCV_poly)\n",
    "print(\"MSE model_ridgeCV_poly:\", mse_ridgeCV_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9ced22",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bba9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree\n",
    "dt = DecisionTreeRegressor(random_state=3).fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78352981",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
    "print(\"MSE model_dt:\", mse_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850fba43",
   "metadata": {},
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f285cc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regressor\n",
    "rf = RandomForestRegressor(max_depth=15, random_state=3)\n",
    "rf.fit(X_train, y_train.reshape(len(y_train)))\n",
    "\n",
    "y_pred_rf = rf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde9826e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "print(\"MSE model_rf:\", mse_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb97249d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
